diff --git a/.gitignore b/.gitignore
index ebdb1ae..d35f6fc 100644
--- a/.gitignore
+++ b/.gitignore
@@ -1,7 +1,12 @@
 # Ignore vscode
 /.vscode
 /DB
-/models
+/DB1
+*.DS_Store
+ALL_DOCUMENTS/
+SOURCE_DOCUMENTS/
+SOURCE_DOCUMENTS_EXTERNAL/
+AutoGPTQ/
 
 # Byte-compiled / optimized / DLL files
 __pycache__/
@@ -163,7 +168,3 @@ cython_debug/
 #  and can be added to the global gitignore or merged into this file.  For a more nuclear
 #  option (not recommended) you can uncomment the following to ignore the entire idea folder.
 .idea/
-
-#MacOS
-.DS_Store
-SOURCE_DOCUMENTS/.DS_Store
\ No newline at end of file
diff --git a/SOURCE_DOCUMENTS/Orca_paper.pdf b/SOURCE_DOCUMENTS/Orca_paper.pdf
deleted file mode 100644
index 6983cb0..0000000
Binary files a/SOURCE_DOCUMENTS/Orca_paper.pdf and /dev/null differ
diff --git a/constants.py b/constants.py
index cf27985..e58a662 100644
--- a/constants.py
+++ b/constants.py
@@ -5,9 +5,6 @@ from chromadb.config import Settings
 
 # https://python.langchain.com/en/latest/modules/indexes/document_loaders/examples/excel.html?highlight=xlsx#microsoft-excel
 from langchain.document_loaders import CSVLoader, PDFMinerLoader, TextLoader, UnstructuredExcelLoader, Docx2txtLoader
-from langchain.document_loaders import UnstructuredFileLoader, UnstructuredMarkdownLoader
-from langchain.document_loaders import UnstructuredHTMLLoader
-
 
 # load_dotenv()
 ROOT_DIRECTORY = os.path.dirname(os.path.realpath(__file__))
@@ -17,39 +14,20 @@ SOURCE_DIRECTORY = f"{ROOT_DIRECTORY}/SOURCE_DOCUMENTS"
 
 PERSIST_DIRECTORY = f"{ROOT_DIRECTORY}/DB"
 
-MODELS_PATH = "./models"
-
 # Can be changed to a specific number
 INGEST_THREADS = os.cpu_count() or 8
 
 # Define the Chroma settings
 CHROMA_SETTINGS = Settings(
-    anonymized_telemetry=False,
-    is_persistent=True,
+    chroma_db_impl="duckdb+parquet", persist_directory=PERSIST_DIRECTORY, anonymized_telemetry=False
 )
 
-# Context Window and Max New Tokens
-CONTEXT_WINDOW_SIZE = 8096
-MAX_NEW_TOKENS = CONTEXT_WINDOW_SIZE  # int(CONTEXT_WINDOW_SIZE/4)
-
-#### If you get a "not enough space in the buffer" error, you should reduce the values below, start with half of the original values and keep halving the value until the error stops appearing
-
-N_GPU_LAYERS = 100  # Llama-2-70B has 83 layers
-N_BATCH = 512
-
-### From experimenting with the Llama-2-7B-Chat-GGML model on 8GB VRAM, these values work:
-# N_GPU_LAYERS = 20
-# N_BATCH = 512
-
-
 # https://python.langchain.com/en/latest/_modules/langchain/document_loaders/excel.html#UnstructuredExcelLoader
 DOCUMENT_MAP = {
-    ".html": UnstructuredHTMLLoader,
     ".txt": TextLoader,
-    ".md": UnstructuredMarkdownLoader,
+    ".md": TextLoader,
     ".py": TextLoader,
-    # ".pdf": PDFMinerLoader,
-    ".pdf": UnstructuredFileLoader,
+    ".pdf": PDFMinerLoader,
     ".csv": CSVLoader,
     ".xls": UnstructuredExcelLoader,
     ".xlsx": UnstructuredExcelLoader,
@@ -58,142 +36,7 @@ DOCUMENT_MAP = {
 }
 
 # Default Instructor Model
-EMBEDDING_MODEL_NAME = "hkunlp/instructor-large"  # Uses 1.5 GB of VRAM (High Accuracy with lower VRAM usage)
-
-####
-#### OTHER EMBEDDING MODEL OPTIONS
-####
-
-# EMBEDDING_MODEL_NAME = "hkunlp/instructor-xl" # Uses 5 GB of VRAM (Most Accurate of all models)
-# EMBEDDING_MODEL_NAME = "intfloat/e5-large-v2" # Uses 1.5 GB of VRAM (A little less accurate than instructor-large)
-# EMBEDDING_MODEL_NAME = "intfloat/e5-base-v2" # Uses 0.5 GB of VRAM (A good model for lower VRAM GPUs)
-# EMBEDDING_MODEL_NAME = "all-MiniLM-L6-v2" # Uses 0.2 GB of VRAM (Less accurate but fastest - only requires 150mb of vram)
-
-####
-#### MULTILINGUAL EMBEDDING MODELS
-####
-
-# EMBEDDING_MODEL_NAME = "intfloat/multilingual-e5-large" # Uses 2.5 GB of VRAM
-# EMBEDDING_MODEL_NAME = "intfloat/multilingual-e5-base" # Uses 1.2 GB of VRAM
-
-
-#### SELECT AN OPEN SOURCE LLM (LARGE LANGUAGE MODEL)
-# Select the Model ID and model_basename
-# load the LLM for generating Natural Language responses
-
-#### GPU VRAM Memory required for LLM Models (ONLY) by Billion Parameter value (B Model)
-#### Does not include VRAM used by Embedding Models - which use an additional 2GB-7GB of VRAM depending on the model.
-####
-#### (B Model)   (float32)    (float16)    (GPTQ 8bit)         (GPTQ 4bit)
-####    7b         28 GB        14 GB       7 GB - 9 GB        3.5 GB - 5 GB
-####    13b        52 GB        26 GB       13 GB - 15 GB      6.5 GB - 8 GB
-####    32b        130 GB       65 GB       32.5 GB - 35 GB    16.25 GB - 19 GB
-####    65b        260.8 GB     130.4 GB    65.2 GB - 67 GB    32.6 GB -  - 35 GB
-
-# MODEL_ID = "TheBloke/Llama-2-7B-Chat-GGML"
-# MODEL_BASENAME = "llama-2-7b-chat.ggmlv3.q4_0.bin"
-
-####
-#### (FOR GGUF MODELS)
-####
-
-# MODEL_ID = "TheBloke/Llama-2-13b-Chat-GGUF"
-# MODEL_BASENAME = "llama-2-13b-chat.Q4_K_M.gguf"
-
-# MODEL_ID = "TheBloke/Llama-2-7b-Chat-GGUF"
-# MODEL_BASENAME = "llama-2-7b-chat.Q4_K_M.gguf"
-
-# MODEL_ID = "QuantFactory/Meta-Llama-3-8B-Instruct-GGUF"
-# MODEL_BASENAME = "Meta-Llama-3-8B-Instruct.Q4_K_M.gguf"
-
-# LLAMA 3 # use for Apple Silicon
-MODEL_ID = "meta-llama/Meta-Llama-3-8B-Instruct"
-MODEL_BASENAME = None
-
-# LLAMA 3 # use for NVIDIA GPUs
-# MODEL_ID = "unsloth/llama-3-8b-bnb-4bit"
-# MODEL_BASENAME = None
-
-# MODEL_ID = "TheBloke/Mistral-7B-Instruct-v0.1-GGUF"
-# MODEL_BASENAME = "mistral-7b-instruct-v0.1.Q8_0.gguf"
-
-# MODEL_ID = "TheBloke/Llama-2-70b-Chat-GGUF"
-# MODEL_BASENAME = "llama-2-70b-chat.Q4_K_M.gguf"
-
-####
-#### (FOR HF MODELS)
-####
-
-# MODEL_ID = "NousResearch/Llama-2-7b-chat-hf"
-# MODEL_BASENAME = None
-# MODEL_ID = "TheBloke/vicuna-7B-1.1-HF"
-# MODEL_BASENAME = None
-# MODEL_ID = "TheBloke/Wizard-Vicuna-7B-Uncensored-HF"
-# MODEL_ID = "TheBloke/guanaco-7B-HF"
-# MODEL_ID = 'NousResearch/Nous-Hermes-13b' # Requires ~ 23GB VRAM. Using STransformers
-# alongside will 100% create OOM on 24GB cards.
-# llm = load_model(device_type, model_id=model_id)
-
-####
-#### (FOR GPTQ QUANTIZED) Select a llm model based on your GPU and VRAM GB. Does not include Embedding Models VRAM usage.
-####
-
-##### 48GB VRAM Graphics Cards (RTX 6000, RTX A6000 and other 48GB VRAM GPUs) #####
-
-### 65b GPTQ LLM Models for 48GB GPUs (*** With best embedding model: hkunlp/instructor-xl ***)
-# MODEL_ID = "TheBloke/guanaco-65B-GPTQ"
-# MODEL_BASENAME = "model.safetensors"
-# MODEL_ID = "TheBloke/Airoboros-65B-GPT4-2.0-GPTQ"
-# MODEL_BASENAME = "model.safetensors"
-# MODEL_ID = "TheBloke/gpt4-alpaca-lora_mlp-65B-GPTQ"
-# MODEL_BASENAME = "model.safetensors"
-# MODEL_ID = "TheBloke/Upstage-Llama1-65B-Instruct-GPTQ"
-# MODEL_BASENAME = "model.safetensors"
-
-##### 24GB VRAM Graphics Cards (RTX 3090 - RTX 4090 (35% Faster) - RTX A5000 - RTX A5500) #####
-
-### 13b GPTQ Models for 24GB GPUs (*** With best embedding model: hkunlp/instructor-xl ***)
-# MODEL_ID = "TheBloke/Wizard-Vicuna-13B-Uncensored-GPTQ"
-# MODEL_BASENAME = "Wizard-Vicuna-13B-Uncensored-GPTQ-4bit-128g.compat.no-act-order.safetensors"
-# MODEL_ID = "TheBloke/vicuna-13B-v1.5-GPTQ"
-# MODEL_BASENAME = "model.safetensors"
-# MODEL_ID = "TheBloke/Nous-Hermes-13B-GPTQ"
-# MODEL_BASENAME = "nous-hermes-13b-GPTQ-4bit-128g.no-act.order"
-# MODEL_ID = "TheBloke/WizardLM-13B-V1.2-GPTQ"
-# MODEL_BASENAME = "gptq_model-4bit-128g.safetensors
-
-### 30b GPTQ Models for 24GB GPUs (*** Requires using intfloat/e5-base-v2 instead of hkunlp/instructor-large as embedding model ***)
-# MODEL_ID = "TheBloke/Wizard-Vicuna-30B-Uncensored-GPTQ"
-# MODEL_BASENAME = "Wizard-Vicuna-30B-Uncensored-GPTQ-4bit--1g.act.order.safetensors"
-# MODEL_ID = "TheBloke/WizardLM-30B-Uncensored-GPTQ"
-# MODEL_BASENAME = "WizardLM-30B-Uncensored-GPTQ-4bit.act-order.safetensors"
-
-##### 8-10GB VRAM Graphics Cards (RTX 3080 - RTX 3080 Ti - RTX 3070 Ti - 3060 Ti - RTX 2000 Series, Quadro RTX 4000, 5000, 6000) #####
-### (*** Requires using intfloat/e5-small-v2 instead of hkunlp/instructor-large as embedding model ***)
-
-### 7b GPTQ Models for 8GB GPUs
-# MODEL_ID = "TheBloke/Wizard-Vicuna-7B-Uncensored-GPTQ"
-# MODEL_BASENAME = "Wizard-Vicuna-7B-Uncensored-GPTQ-4bit-128g.no-act.order.safetensors"
-# MODEL_ID = "TheBloke/WizardLM-7B-uncensored-GPTQ"
-# MODEL_BASENAME = "WizardLM-7B-uncensored-GPTQ-4bit-128g.compat.no-act-order.safetensors"
-# MODEL_ID = "TheBloke/wizardLM-7B-GPTQ"
-# MODEL_BASENAME = "wizardLM-7B-GPTQ-4bit.compat.no-act-order.safetensors"
-
-####
-#### (FOR GGML) (Quantized cpu+gpu+mps) models - check if they support llama.cpp
-####
-
-# MODEL_ID = "TheBloke/wizard-vicuna-13B-GGML"
-# MODEL_BASENAME = "wizard-vicuna-13B.ggmlv3.q4_0.bin"
-# MODEL_BASENAME = "wizard-vicuna-13B.ggmlv3.q6_K.bin"
-# MODEL_BASENAME = "wizard-vicuna-13B.ggmlv3.q2_K.bin"
-# MODEL_ID = "TheBloke/orca_mini_3B-GGML"
-# MODEL_BASENAME = "orca-mini-3b.ggmlv3.q4_0.bin"
-
-####
-#### (FOR AWQ QUANTIZED) Select a llm model based on your GPU and VRAM GB. Does not include Embedding Models VRAM usage.
-### (*** MODEL_BASENAME is not actually used but have to contain .awq so the correct model loading is used ***)
-### (*** Compute capability 7.5 (sm75) and CUDA Toolkit 11.8+ are required ***)
-####
-# MODEL_ID = "TheBloke/Llama-2-7B-Chat-AWQ"
-# MODEL_BASENAME = "model.safetensors.awq"
+EMBEDDING_MODEL_NAME = "hkunlp/instructor-large"
+# You can also choose a smaller model, don't forget to change HuggingFaceInstructEmbeddings
+# to HuggingFaceEmbeddings in both ingest.py and run_localGPT.py
+# EMBEDDING_MODEL_NAME = "all-MiniLM-L6-v2"
diff --git a/ingest.py b/ingest.py
index 5e61627..68392bd 100644
--- a/ingest.py
+++ b/ingest.py
@@ -3,11 +3,10 @@ import os
 from concurrent.futures import ProcessPoolExecutor, ThreadPoolExecutor, as_completed
 
 import click
-import torch
 from langchain.docstore.document import Document
+from langchain.embeddings import HuggingFaceInstructEmbeddings
 from langchain.text_splitter import Language, RecursiveCharacterTextSplitter
 from langchain.vectorstores import Chroma
-from utils import get_embeddings
 
 from constants import (
     CHROMA_SETTINGS,
@@ -19,28 +18,15 @@ from constants import (
 )
 
 
-def file_log(logentry):
-    file1 = open("file_ingest.log", "a")
-    file1.write(logentry + "\n")
-    file1.close()
-    print(logentry + "\n")
-
-
 def load_single_document(file_path: str) -> Document:
     # Loads a single document from a file path
-    try:
-        file_extension = os.path.splitext(file_path)[1]
-        loader_class = DOCUMENT_MAP.get(file_extension)
-        if loader_class:
-            file_log(file_path + " loaded.")
-            loader = loader_class(file_path)
-        else:
-            file_log(file_path + " document type is undefined.")
-            raise ValueError("Document type is undefined")
-        return loader.load()[0]
-    except Exception as ex:
-        file_log("%s loading error: \n%s" % (file_path, ex))
-        return None
+    file_extension = os.path.splitext(file_path)[1]
+    loader_class = DOCUMENT_MAP.get(file_extension)
+    if loader_class:
+        loader = loader_class(file_path)
+    else:
+        raise ValueError("Document type is undefined")
+    return loader.load()[0]
 
 
 def load_document_batch(filepaths):
@@ -50,25 +36,20 @@ def load_document_batch(filepaths):
         # load files
         futures = [exe.submit(load_single_document, name) for name in filepaths]
         # collect data
-        if futures is None:
-            file_log(name + " failed to submit")
-            return None
-        else:
-            data_list = [future.result() for future in futures]
-            # return data and file paths
-            return (data_list, filepaths)
+        data_list = [future.result() for future in futures]
+        # return data and file paths
+        return (data_list, filepaths)
 
 
 def load_documents(source_dir: str) -> list[Document]:
-    # Loads all documents from the source documents directory, including nested folders
+    # Loads all documents from the source documents directory
+    all_files = os.listdir(source_dir)
     paths = []
-    for root, _, files in os.walk(source_dir):
-        for file_name in files:
-            print("Importing: " + file_name)
-            file_extension = os.path.splitext(file_name)[1]
-            source_file_path = os.path.join(root, file_name)
-            if file_extension in DOCUMENT_MAP.keys():
-                paths.append(source_file_path)
+    for file_path in all_files:
+        file_extension = os.path.splitext(file_path)[1]
+        source_file_path = os.path.join(source_dir, file_path)
+        if file_extension in DOCUMENT_MAP.keys():
+            paths.append(source_file_path)
 
     # Have at least one worker and at most INGEST_THREADS workers
     n_workers = min(INGEST_THREADS, max(len(paths), 1))
@@ -81,21 +62,13 @@ def load_documents(source_dir: str) -> list[Document]:
             # select a chunk of filenames
             filepaths = paths[i : (i + chunksize)]
             # submit the task
-            try:
-                future = executor.submit(load_document_batch, filepaths)
-            except Exception as ex:
-                file_log("executor task failed: %s" % (ex))
-                future = None
-            if future is not None:
-                futures.append(future)
+            future = executor.submit(load_document_batch, filepaths)
+            futures.append(future)
         # process all results
         for future in as_completed(futures):
             # open the file and load the data
-            try:
-                contents, _ = future.result()
-                docs.extend(contents)
-            except Exception as ex:
-                file_log("Exception: %s" % (ex))
+            contents, _ = future.result()
+            docs.extend(contents)
 
     return docs
 
@@ -104,19 +77,19 @@ def split_documents(documents: list[Document]) -> tuple[list[Document], list[Doc
     # Splits documents for correct Text Splitter
     text_docs, python_docs = [], []
     for doc in documents:
-        if doc is not None:
-            file_extension = os.path.splitext(doc.metadata["source"])[1]
-            if file_extension == ".py":
-                python_docs.append(doc)
-            else:
-                text_docs.append(doc)
+        file_extension = os.path.splitext(doc.metadata["source"])[1]
+        if file_extension == ".py":
+            python_docs.append(doc)
+        else:
+            text_docs.append(doc)
+
     return text_docs, python_docs
 
 
 @click.command()
 @click.option(
     "--device_type",
-    default="cuda" if torch.cuda.is_available() else "cpu",
+    default="cuda",
     type=click.Choice(
         [
             "cpu",
@@ -149,23 +122,24 @@ def main(device_type):
     text_documents, python_documents = split_documents(documents)
     text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
     python_splitter = RecursiveCharacterTextSplitter.from_language(
-        language=Language.PYTHON, chunk_size=880, chunk_overlap=200
+        language=Language.PYTHON, chunk_size=1000, chunk_overlap=200
     )
     texts = text_splitter.split_documents(text_documents)
     texts.extend(python_splitter.split_documents(python_documents))
     logging.info(f"Loaded {len(documents)} documents from {SOURCE_DIRECTORY}")
     logging.info(f"Split into {len(texts)} chunks of text")
 
-    """
-    (1) Chooses an appropriate langchain library based on the enbedding model name.  Matching code is contained within fun_localGPT.py.
-    
-    (2) Provides additional arguments for instructor and BGE models to improve results, pursuant to the instructions contained on
-    their respective huggingface repository, project page or github repository.
-    """
-
-    embeddings = get_embeddings(device_type)
+    # Create embeddings
+    embeddings = HuggingFaceInstructEmbeddings(
+        model_name=EMBEDDING_MODEL_NAME,
+        model_kwargs={"device": device_type},
+    )
+    # change the embedding type here if you are running into issues.
+    # These are much smaller embeddings and will work for most appications
+    # If you use HuggingFaceEmbeddings, make sure to also use the same in the
+    # run_localGPT.py file.
 
-    logging.info(f"Loaded embeddings from {EMBEDDING_MODEL_NAME}")
+    # embeddings = HuggingFaceEmbeddings(model_name=EMBEDDING_MODEL_NAME)
 
     db = Chroma.from_documents(
         texts,
@@ -173,6 +147,8 @@ def main(device_type):
         persist_directory=PERSIST_DIRECTORY,
         client_settings=CHROMA_SETTINGS,
     )
+    db.persist()
+    db = None
 
 
 if __name__ == "__main__":
diff --git a/localGPTUI/localGPTUI.py b/localGPTUI/localGPTUI.py
index fafea20..b0651e0 100755
--- a/localGPTUI/localGPTUI.py
+++ b/localGPTUI/localGPTUI.py
@@ -12,8 +12,6 @@ sys.path.append(os.path.join(os.path.dirname(__file__), ".."))
 app = Flask(__name__)
 app.secret_key = "LeafmanZSecretKey"
 
-API_HOST = "http://localhost:5110/api"
-
 
 # PAGES #
 @app.route("/", methods=["GET", "POST"])
@@ -22,20 +20,25 @@ def home_page():
         if "user_prompt" in request.form:
             user_prompt = request.form["user_prompt"]
             print(f"User Prompt: {user_prompt}")
-
-            main_prompt_url = f"{API_HOST}/prompt_route"
+            if len(user_prompt.split(";")) == 2:
+                main_prompt_url = "http://localhost:5110/api/web_route"
+            elif len(user_prompt.split(";")) >= 3:
+                main_prompt_url = "http://localhost:5110/api/sql_route"
+            else:
+                main_prompt_url = "http://localhost:5110/api/prompt_route"
+                    
             response = requests.post(main_prompt_url, data={"user_prompt": user_prompt})
             print(response.status_code)  # print HTTP response status code for debugging
             if response.status_code == 200:
                 # print(response.json())  # Print the JSON data from the response
                 return render_template("home.html", show_response_modal=True, response_dict=response.json())
         elif "documents" in request.files:
-            delete_source_url = f"{API_HOST}/delete_source"  # URL of the /api/delete_source endpoint
+            delete_source_url = "http://localhost:5110/api/delete_source"  # URL of the /api/delete_source endpoint
             if request.form.get("action") == "reset":
                 response = requests.get(delete_source_url)
 
-            save_document_url = f"{API_HOST}/save_document"
-            run_ingest_url = f"{API_HOST}/run_ingest"  # URL of the /api/run_ingest endpoint
+            save_document_url = "http://localhost:5110/api/save_document"
+            run_ingest_url = "http://localhost:5110/api/run_ingest"  # URL of the /api/run_ingest endpoint
             files = request.files.getlist("documents")
             for file in files:
                 print(file.filename)
@@ -56,17 +59,56 @@ def home_page():
         response_dict={"Prompt": "None", "Answer": "None", "Sources": [("ewf", "wef")]},
     )
 
+# @app.route("/sql", methods=["GET", "POST"])
+# def sql():
+#     if request.method == "POST":
+#         if "user_prompt" in request.form:
+#             user_prompt = request.form["user_prompt"]
+#             print(f"User Prompt: {user_prompt}")
+
+#             main_prompt_url = "http://localhost:5110/api/sql_route"
+#             response = requests.post(main_prompt_url, data={"user_prompt": user_prompt})
+#             print(response.status_code)  # print HTTP response status code for debugging
+#             if response.status_code == 200:
+#                 # print(response.json())  # Print the JSON data from the response
+#                 return render_template("home.html", show_response_modal=True, response_dict=response.json())
+
+#     # Display the form for GET request
+#     return render_template(
+#         "home.html",
+#         show_response_modal=False,
+#         response_dict={"Prompt": "None", "Answer": "None", "Sources": [("ewf", "wef")]},
+#     )
+
+# @app.route("/web", methods=["GET", "POST"])
+# def web():
+#     if request.method == "POST":
+#         if "user_prompt" in request.form:
+#             user_prompt = request.form["user_prompt"]
+#             print(f"User Prompt: {user_prompt}")
+
+#             main_prompt_url = "http://localhost:5110/api/web_route"
+#             response = requests.post(main_prompt_url, data={"user_prompt": user_prompt})
+#             print(response.status_code)  # print HTTP response status code for debugging
+#             if response.status_code == 200:
+#                 # print(response.json())  # Print the JSON data from the response
+#                 return render_template("home.html", show_response_modal=True, response_dict=response.json())
+
+#     # Display the form for GET request
+#     return render_template(
+#         "home.html",
+#         show_response_modal=False,
+#         response_dict={"Prompt": "None", "Answer": "None", "Sources": [("ewf", "wef")]},
+#     )
+
 
 if __name__ == "__main__":
     parser = argparse.ArgumentParser()
-    parser.add_argument("--port", type=int, default=5111, help="Port to run the UI on. Defaults to 5111.")
-    parser.add_argument(
-        "--host",
-        type=str,
-        default="127.0.0.1",
-        help="Host to run the UI on. Defaults to 127.0.0.1. "
-        "Set to 0.0.0.0 to make the UI externally "
-        "accessible from other devices.",
-    )
+    parser.add_argument("--port", type=int, default=80,
+                        help="Port to run the UI on. Defaults to 5111.")
+    parser.add_argument("--host", type=str, default="0.0.0.0",
+                        help="Host to run the UI on. Defaults to 127.0.0.1. "
+                             "Set to 0.0.0.0 to make the UI externally "
+                             "accessible from other devices.")
     args = parser.parse_args()
-    app.run(debug=False, host=args.host, port=args.port)
+    app.run(debug=True, host=args.host, port=args.port)
diff --git a/localGPTUI/static/document_examples/constitution.pdf b/localGPTUI/static/document_examples/constitution.pdf
deleted file mode 100644
index 447cb0d..0000000
Binary files a/localGPTUI/static/document_examples/constitution.pdf and /dev/null differ
diff --git a/localGPTUI/templates/home.html b/localGPTUI/templates/home.html
index 2da2034..91d0e3f 100644
--- a/localGPTUI/templates/home.html
+++ b/localGPTUI/templates/home.html
@@ -6,16 +6,15 @@
     <meta name="viewport" content="width=device-width, initial-scale=1.0" />
     <!-- Bootstrap CSS -->
 
-
-   <link rel="stylesheet" href="static/dependencies/bootstrap-5.1.3-dist/css/bootstrap.min.css" />
-<!-- Font Awesome CSS -->
-<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" />
-
-   <script src="https://code.jquery.com/jquery-3.6.0.js" integrity="sha256-H+K7U5CnXl1h5ywQfKtSj8PCmoN9aaq30gDh27Xc0jk=" crossorigin="anonymous"></script>
-    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.1.3/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha384-1BmE4kWBq78iYhFldvKuhfTAU6auU8tT94WrHftjDbrCEXSU1oBoqyl2QvZ6jIW3" crossorigin="anonymous">
-    <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.1.3/dist/js/bootstrap.bundle.min.js" integrity="sha384-ka7Sk0Gln4gmtz2MlQnikT1wXgYsOg+OMhuP+IlRH9sENBO0LRn5q+8nbTov4+1p" crossorigin="anonymous"></script>
-
-    <title> ChatBot</title>
+    <link rel="icon" type="image" href="static\social_icons\favicon.png" />
+    <script src="static\dependencies\jquery\3.6.0\jquery.min.js"></script>
+    <script src="static\dependencies\bootstrap-5.1.3-dist\js\bootstrap.min.js"></script>
+    <link
+      rel="stylesheet"
+      href="static\dependencies\bootstrap-5.1.3-dist\css\bootstrap.min.css"
+    />
+
+    <title>localGPT</title>
   </head>
   <script>
     function openFileSelection() {
@@ -39,7 +38,7 @@
       $("#responseModal").modal("show");
 
       // Submit the form after a short delay to allow the modal to open
-   setTimeout(function () {
+      setTimeout(function () {
         document.getElementById("promptForm").submit();
       }, 5);
     }
@@ -75,73 +74,12 @@
   {% endif %}
   <style>
     body {
-      background-image: linear-gradient(to left top, #bebebe, #000000);
+      background-image: linear-gradient(to right top, #d91b23, #124feb);
+      height: 100vh;
       display: flex;
-        flex-direction: column;
-        height: 600px;
-        width: 400px;
-        position: fixed;
-        bottom: 0px;
-          right: 30px;
-        border-radius: 8px 8px 0px 0px;
       justify-content: center;
       align-items: center;
     }
-     #chatbotTrigger {
-  cursor: pointer;
-  position: fixed;
-  bottom: 10px;
-  right: 20px;
-  width: 50px;
-  height: auto;
-  font-size: 70px;
-  color:  rgb(109, 131, 243);
-}
-   .appContainer {
-        display: none;
-        flex-direction: column;
-        height: 600px;
-        width: 400px;
-        position: fixed;
-        bottom: 0px;
-        right: 65px;
-        border-radius: 8px 8px 0px 0px;
-      }
-      .appContainer .header {
-        height: 48px;
-        min-height: 48px;
-        flex-grow: 0;
-        background-color: rgb(109, 131, 243);
-        border-radius: 10px;
-        display: flex;
-        align-items: center;
-        justify-content: center;
-        font-size: 20px; /* Adjust the value to set the desired font size */
-       margin: 0;
-      }
-
-            .appContainer .content {
-        flex-grow: 1;
-        background-color: rgb(193, 193, 180);
-        5
-      }
-    .modal {
-  position: absolute;
-  top: 50%;
-  left: 50%;
-  transform: translate(-50%, -50%);
-}
-
-.modal-dialog{
-  margin:auto;
-  width: fit-content;
-}
-
-.modal-content {
-  width:fit-content;
-  max-width: 800px; /* Adjust max-width as needed */
-}
-
 
     .search {
       box-shadow:
@@ -150,16 +88,11 @@
       background-color: #fff;
       padding: 4px;
       border-radius: 5px;
-      width:390px;
-      position: fixed;
-      bottom: 0;
-      right: 10;
     }
 
     ::placeholder {
       color: #eee;
       opacity: 1;
-      width: 100;
     }
 
     .search-2 {
@@ -168,11 +101,11 @@
     }
 
     .search-2 input {
-   height: 45px;
+      height: 45px;
       border: none;
       width: 100%;
-      padding-left: 0px;
-      padding-right: 20px;
+      padding-left: 100px;
+      padding-right: 200px;
     }
 
     .search-2 input:focus {
@@ -191,18 +124,18 @@
 
     .search-2 button {
       position: absolute;
-      right:0px;
+      right: 100px;
       top: 0px;
       border: none;
       height: 45px;
-      background-color: rgb(111, 143, 247);
+      background-color: red;
       color: #fff;
       width: 90px;
       border-radius: 4px;
     }
 
     .search-2 button:hover {
-      background: #5304d2;
+      background: #d2042d;
       color: #ffffff;
       transition: all 0.2s ease;
       cursor: pointer;
@@ -211,10 +144,10 @@
     .search-2 .upload_button {
       position: absolute;
       right: 1px;
-     top: 0px;
+      top: 0px;
       border: none;
       height: 45px;
-      background-color: rgb(111, 143, 247);;
+      background-color: red;
       color: #fff;
       width: 90px;
       border-radius: 4px;
@@ -239,7 +172,7 @@
     .default_button {
       border: none;
       height: 45px;
-      background-color: rgb(111, 143, 247);;
+      background-color: red;
       color: #fff;
       width: 90px;
       border-radius: 4px;
@@ -254,7 +187,7 @@
 
     .upload_button:hover {
       background: #d2042d;
-        color: #ffffff;
+      color: #ffffff;
       transition: all 0.2s ease;
       cursor: pointer;
     }
@@ -297,7 +230,7 @@
       .search-2 button {
         height: 37px;
         top: 5px;
-         }
+      }
 
       .search-2 .tutorial_button {
         height: 37px;
@@ -339,11 +272,7 @@
     }
   </style>
   <body>
-         <!-- <img src="static/social_icons/chat.png" alt="Chatbot Trigger" id="chatbotTrigger">-->
-   <i class="fas fa-comment"  alt="Chatbot Trigger"  id="chatbotTrigger"></i>
-    <div class="appContainer" id="chatbotContainer" >
-     <div class="header">Chatbot</div>
-      <div class="content">
+    <div class="container">
       <div class="search">
         <div class="row">
           <div class="col-md-12">
@@ -353,14 +282,14 @@
                 action="{{ url_for('home_page') }}"
                 method="POST"
               >
-            <!--    <button
+                <button
                   type="button"
                   class="tutorial_button"
                   data-bs-toggle="modal"
                   data-bs-target="#tutorial_modal"
                 >
                   Tutorial
-                </button>-->
+                </button>
                 <i class="bx bxs-map"></i>
                 <input
                   type="text"
@@ -383,15 +312,15 @@
                   name="documents"
                   id="fileInput"
                   style="display: none"
-                     multiple
+                  multiple
                 />
-               <!-- <button
+                <button
                   type="button"
                   class="upload_button"
                   onclick="openFileSelection()"
                 >
                   Upload
-               </button>-->
+                </button>
               </form>
             </div>
           </div>
@@ -409,25 +338,6 @@
             submitPromptForm(); // Call the function to show the modal
           }
         });
-        document.addEventListener("DOMContentLoaded", function() {
-  // Image trigger element
-  var chatbotTrigger = document.getElementById("chatbotTrigger");
-
-  // Chatbot container element
-  var chatbotContainer = document.getElementById("chatbotContainer");
-
-
-  // Event listener for image click
-  chatbotTrigger.addEventListener("click", function() {
-       // Show/hide the chatbot container
-    if (chatbotContainer.style.display === "none") {
-      chatbotContainer.style.display = "flex"; // or "block" depending on your layout
-    } else {
-      chatbotContainer.style.display = "none";
-    }
-  });
-});
-
     </script>
 
     <!-- Chat Response Modal -->
@@ -436,7 +346,6 @@
       id="response_modal"
       tabindex="-1"
       aria-labelledby="response_modal"
-     data-bs-backdrop="false"
       aria-hidden="true"
     >
       <div class="modal-dialog modal-xl modal-dialog-centered">
@@ -461,7 +370,7 @@
               {{response_dict['Answer']}}
             </p>
 
-           <!-- <strong>Sources</strong>
+            <strong>Sources</strong>
             <div
               class="accordion"
               id="accordionExample"
@@ -469,7 +378,7 @@
             >
               {% for item in response_dict['Sources'] %}
               <div class="accordion-item">
- <h2
+                <h2
                   class="accordion-header text-center"
                   id="heading{{ loop.index0 }}"
                 >
@@ -482,7 +391,7 @@
                     aria-controls="collapse{{ loop.index0 }}"
                   >
                     {{ item[0] }}
-
+                    <!-- Source -->
                   </button>
                 </h2>
                 <div
@@ -500,19 +409,19 @@
                     "
                   >
                     {{ item[1] }}
-
+                    <!-- Content -->
                   </div>
                 </div>
               </div>
               {% endfor %}
-            </div> -->
+            </div>
           </div>
           <div class="modal-footer">
             <button
               type="button"
               class="default_button"
               data-bs-dismiss="modal"
-                 >
+            >
               Close
             </button>
           </div>
@@ -527,7 +436,7 @@
       tabindex="-1"
       aria-labelledby="responseModalLabel"
       aria-hidden="true"
-      data-bs-backdrop="false"
+      data-bs-backdrop="static"
       data-bs-keyboard="false"
     >
       <div class="modal-dialog modal-dialog-centered">
@@ -555,7 +464,7 @@
       aria-labelledby="uploadModalLabel"
       aria-hidden="true"
     >
-   <div class="modal-dialog modal-dialog-centered">
+      <div class="modal-dialog modal-dialog-centered">
         <div class="modal-content">
           <div class="modal-header justify-content-center">
             <h5
@@ -598,7 +507,7 @@
             >
               Add
             </button>
-              <button
+            <button
               type="submit"
               name="button_clicked"
               value="reset_document"
@@ -641,7 +550,7 @@
       class="modal fade"
       id="tutorial_modal"
       tabindex="-1"
-  aria-labelledby="tutorial_modal"
+      aria-labelledby="tutorial_modal"
       aria-hidden="true"
     >
       <div class="modal-dialog modal-xl modal-dialog-centered">
@@ -652,24 +561,21 @@
               id="tutorial_modal"
               style="color: #292b2c"
             >
-              Tutorial
+              IncredGPT
             </h5>
           </div>
           <div class="modal-body" style="color: #292b2c">
             <strong style="margin-left: 5%; text-align: left">About</strong>
             <br />
             <p style="text-align: justify; margin-left: 5%; margin-right: 5%">
-              Introducing an application that empowers users to
-              leverage the capabilities of a language model, even in the absence
-              of an internet connection. This advanced tool serves as an
+              Introducing a cutting-edge application that empowers users to
+              leverage the capabilities of a language model. This advanced tool serves as an
               indispensable resource for accessing information beyond the
               confines of traditional language model tools such as chatGPT.
               <br /><br />
-              One of the key advantages of this application is the preservation
-              of data control. This feature becomes particularly valuable when
-              handling sensitive data that must remain within the confines of an
-              organization or personal documents that warrant utmost
-              confidentiality, eliminating the need to transmit information
+              One of the key advantages of IncredGPT is the preservation
+              of data control. As this llm model runs inside Incred's VPN,
+              it eliminates the need to transmit information
               through third-party channels.
               <br /><br />
               Seamlessly integrating personal documents into the system is
@@ -684,17 +590,77 @@
               utilization. Unlike the resource-intensive retraining processes
               employed by alternative methods, the ingestion of documents within
               this application demands significantly less compute power. This
-               efficiency optimization allows for a streamlined user experience,
+              efficiency optimization allows for a streamlined user experience,
               saving both time and computational resources.
               <br /><br />
               Discover the unparalleled capabilities of this technical marvel,
               as it enables users to tap into the full potential of language
-              models, all while operating offline. Experience a new era of
+              models. Experience a new era of
               information access, bolstering productivity and expanding
               possibilities. Embrace this powerful tool and unlock the true
               potential of your data today.
             </p>
             <br />
+            <strong style="margin-left: 5%; text-align: left">Searching</strong>
+            <p
+              style="
+                line-height: 1.75;
+                margin-left: 5%;
+                margin-right: 5%;
+                text-align: left;
+              "
+            >
+              1. In order to ask a question, type a question into the search bar
+              like:
+              
+              What does application_created_datetime in table staging_application denote?
+              <br />
+              2. Hit enter on your keyboard or click
+              <span style="background-color: red; color: white">Search</span>
+              <br />
+              3. Wait while the LLM model consumes the prompt and prepares the
+              answer.
+              <br />
+              4. Once done, it will print the answer and the 4 sources it used
+              as context from your documents; you can then ask another question
+              without re-running the script, just wait for the prompt again.
+            </p>
+            <br />
+            <strong style="margin-left: 5%; text-align: left">How to Use</strong>
+            <p
+              style="
+                line-height: 1.75;
+                margin-left: 5%;
+                margin-right: 5%;
+                text-align: left;
+              "
+            >
+              It can be used in four ways:
+              <br />
+              1.
+              <span style="background-color: gray; color: white"
+                >Searching in internal documents:</span
+              > In this method, simply type the questions and click Search as mentioned above.
+              <br />
+              2.
+              <span style="background-color: gray; color: white"
+                >Searching in database tables:</span
+              > In this method, type the question, database, schema and table name separated by ; and then click Search. Example:
+              What is average withdrawal amount;pronto;bnk;transactions
+              <br />
+              3. 
+              <span style="background-color: gray; color: white"
+                >Searching from online document or web page:</span
+              > In this method, type the question and the web url separated by ; and then click Search. Example:
+              What are 5 ways your startup can survive an economic downturn?; https://www.incred.com/blog/5-ways-your-startup-can-survive-economic-downturn/
+              <br />
+              4.
+              <span style="background-color: gray; color: white"
+              >Searching in external documents:</span
+            > In this method, first upload the documnt(s) using the method mentioned below. After that type the question prefixed by "E:" and then click Search. Example:
+            E: what is Privilege Leave
+            <br />
+            </p>
             <strong style="margin-left: 5%; text-align: left"
               >Upload Documents</strong
             >
@@ -726,51 +692,22 @@
               4. There will be a short wait time as the documents get ingested
               into the vector database as your new knowledge base.
             </p>
-            <!--
-             <a
-              href="{{ url_for('static', filename='document_examples/constitution.pdf') }}"
+            <a
+              href="static/document_examples/Employee_Handbook.pdf"
               style="margin-left: 5%"
               download
             >
               <button class="default_button" style="width: auto">
-                Example Constitution
+                Example Employee_Handbook
               </button>
             </a>
-            <a href="{{ url_for('static', filename='document_examples/news_articles.zip') }}" download>
+            <a href="static/document_examples/news_articles.zip" download>
               <button class="default_button" style="width: auto">
                 Example News Articles
               </button>
             </a>
-            -->
 
             <br />
-            <strong style="margin-left: 5%; text-align: left">Searching</strong>
-            <p
-              style="
-                line-height: 1.75;
-                margin-left: 5%;
-                margin-right: 5%;
-                text-align: left;
-              "
-            >
-              1. In order to ask a question, type a question into the search bar.
-              <br />
-             <!-- 2.
-              <span style="background-color: gray; color: white"
-                >Give me a summary of the documents</span
-                >
-                <br />-->
-              2. Hit enter on your keyboard or click
-              <span style="background-color: red; color: white">Search</span>
-              <br />
-              3. Wait while the LLM model consumes the prompt and prepares the
-              answer.
-              <br />
-              4. Once done, it will print the answer and the 4 sources it used
-              as context from your documents; you can then ask another question
-              without re-running the script, just wait for the prompt again.
-            </p>
- <br />
           </div>
           <div class="modal-footer justify-content-center">
             <button
@@ -786,4 +723,3 @@
     </div>
   </body>
 </html>
-
diff --git a/requirements.txt b/requirements.txt
index bb48e7f..a43d1ef 100644
--- a/requirements.txt
+++ b/requirements.txt
@@ -1,33 +1,26 @@
 # Natural Language Processing
-langchain==0.0.267
-chromadb==0.4.6
+langchain==0.0.191
+chromadb==0.3.22
+llama-cpp-python==0.1.66
 pdfminer.six==20221105
 InstructorEmbedding
-sentence-transformers==2.2.2
+sentence-transformers
 faiss-cpu
 huggingface_hub
 transformers
-autoawq; sys_platform != 'darwin'
-protobuf==3.20.2; sys_platform != 'darwin'
-protobuf==3.20.2; sys_platform == 'darwin' and platform_machine != 'arm64'
+protobuf==3.20.0; sys_platform != 'darwin'
+protobuf==3.20.0; sys_platform == 'darwin' and platform_machine != 'arm64'
 protobuf==3.20.3; sys_platform == 'darwin' and platform_machine == 'arm64'
-auto-gptq==0.6.0; sys_platform != 'darwin'
+auto-gptq
 docx2txt
-unstructured
-unstructured[pdf]
 
 # Utilities
 urllib3==1.26.6
 accelerate
-bitsandbytes ; sys_platform != 'win32'
-bitsandbytes-windows ; sys_platform == 'win32'
+bitsandbytes
 click
 flask
 requests
 
-# Streamlit related
-streamlit
-Streamlit-extras
-
 # Excel File Manipulation
 openpyxl
diff --git a/run_localGPT.py b/run_localGPT.py
index 185c983..34e861e 100644
--- a/run_localGPT.py
+++ b/run_localGPT.py
@@ -1,45 +1,45 @@
-import os
 import logging
+
 import click
 import torch
-import utils
+from auto_gptq import AutoGPTQForCausalLM
+from huggingface_hub import hf_hub_download
 from langchain.chains import RetrievalQA
 from langchain.embeddings import HuggingFaceInstructEmbeddings
-from langchain.llms import HuggingFacePipeline
-from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler  # for streaming response
-from langchain.callbacks.manager import CallbackManager
-
-callback_manager = CallbackManager([StreamingStdOutCallbackHandler()])
-
-from prompt_template_utils import get_prompt_template
-from utils import get_embeddings
+from langchain.llms import HuggingFacePipeline, LlamaCpp
 
 # from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler
 from langchain.vectorstores import Chroma
 from transformers import (
+    AutoModelForCausalLM,
+    AutoTokenizer,
     GenerationConfig,
+    LlamaForCausalLM,
+    LlamaTokenizer,
     pipeline,
 )
 
-from load_models import (
-    load_quantized_model_awq,
-    load_quantized_model_gguf_ggml,
-    load_quantized_model_qptq,
-    load_full_model,
-)
-
-from constants import (
-    EMBEDDING_MODEL_NAME,
-    PERSIST_DIRECTORY,
-    MODEL_ID,
-    MODEL_BASENAME,
-    MAX_NEW_TOKENS,
-    MODELS_PATH,
-    CHROMA_SETTINGS,
-)
+from constants import CHROMA_SETTINGS, EMBEDDING_MODEL_NAME, PERSIST_DIRECTORY, ROOT_DIRECTORY
+from chromadb.config import Settings
 
-
-def load_model(device_type, model_id, model_basename=None, LOGGING=logging):
+from langchain import OpenAI, SQLDatabase, SQLDatabaseChain
+from langchain.document_loaders import UnstructuredURLLoader
+from langchain.text_splitter import Language, RecursiveCharacterTextSplitter
+# from youtube_transcript_api import YouTubeTranscriptApi
+import os
+from dotenv import load_dotenv
+load_dotenv()
+user_name = os.getenv('user_name')
+user_pwd = os.getenv('user_pwd')
+llm_type = os.getenv('llm_type')
+OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')
+gcp_project = os.getenv('gcp_project')
+host_name = os.getenv('host_name')
+port = 5432
+database = 'rain'
+pg_uri = f"postgresql+psycopg2://{user_name}:{user_pwd}@{host_name}:{port}/{database}"
+
+def load_model(device_type, model_id, model_basename=None):
     """
     Select a model for text generation using the HuggingFace library.
     If you are running this for the first time, it will download a model for you.
@@ -61,17 +61,62 @@ def load_model(device_type, model_id, model_basename=None, LOGGING=logging):
     logging.info("This action can take a few minutes!")
 
     if model_basename is not None:
-        if ".gguf" in model_basename.lower():
-            llm = load_quantized_model_gguf_ggml(model_id, model_basename, device_type, LOGGING)
-            return llm
-        elif ".ggml" in model_basename.lower():
-            model, tokenizer = load_quantized_model_gguf_ggml(model_id, model_basename, device_type, LOGGING)
-        elif ".awq" in model_basename.lower():
-            model, tokenizer = load_quantized_model_awq(model_id, LOGGING)
+        if device_type.lower() in ["cpu", "mps"]:
+            logging.info("Using Llamacpp for quantized models")
+            model_path = hf_hub_download(repo_id=model_id, filename=model_basename)
+            if device_type.lower() == "mps":
+                return LlamaCpp(
+                    model_path=model_path,
+                    n_ctx=2048,
+                    max_tokens=2048,
+                    temperature=0,
+                    repeat_penalty=1.15,
+                    n_gpu_layers=1000,
+                )
+            return LlamaCpp(model_path=model_path, n_ctx=2048, max_tokens=2048, temperature=0, repeat_penalty=1.15)
+
         else:
-            model, tokenizer = load_quantized_model_qptq(model_id, model_basename, device_type, LOGGING)
+            # The code supports all huggingface models that ends with GPTQ and have some variation
+            # of .no-act.order or .safetensors in their HF repo.
+            logging.info("Using AutoGPTQForCausalLM for quantized models")
+
+            if ".safetensors" in model_basename:
+                # Remove the ".safetensors" ending if present
+                model_basename = model_basename.replace(".safetensors", "")
+
+            tokenizer = AutoTokenizer.from_pretrained(model_id, use_fast=True)
+            logging.info("Tokenizer loaded")
+
+            model = AutoGPTQForCausalLM.from_quantized(
+                model_id,
+                model_basename=model_basename,
+                use_safetensors=True,
+                trust_remote_code=True,
+                device="cuda:0",
+                use_triton=False,
+                quantize_config=None,
+            )
+    elif (
+        device_type.lower() == "cuda"
+    ):  # The code supports all huggingface models that ends with -HF or which have a .bin
+        # file in their HF repo.
+        logging.info("Using AutoModelForCausalLM for full models")
+        tokenizer = AutoTokenizer.from_pretrained(model_id)
+        logging.info("Tokenizer loaded")
+
+        model = AutoModelForCausalLM.from_pretrained(
+            model_id,
+            device_map="auto",
+            torch_dtype=torch.float16,
+            low_cpu_mem_usage=True,
+            trust_remote_code=True,
+            # max_memory={0: "15GB"} # Uncomment this line with you encounter CUDA out of memory errors
+        )
+        model.tie_weights()
     else:
-        model, tokenizer = load_full_model(model_id, model_basename, device_type, LOGGING)
+        logging.info("Using LlamaTokenizer")
+        tokenizer = LlamaTokenizer.from_pretrained(model_id)
+        model = LlamaForCausalLM.from_pretrained(model_id)
 
     # Load configuration from the model to avoid warnings
     generation_config = GenerationConfig.from_pretrained(model_id)
@@ -84,9 +129,9 @@ def load_model(device_type, model_id, model_basename=None, LOGGING=logging):
         "text-generation",
         model=model,
         tokenizer=tokenizer,
-        max_length=MAX_NEW_TOKENS,
-        temperature=0.2,
-        # top_p=0.95,
+        max_length=2048,
+        temperature=0,
+        top_p=0.95,
         repetition_penalty=1.15,
         generation_config=generation_config,
     )
@@ -96,80 +141,48 @@ def load_model(device_type, model_id, model_basename=None, LOGGING=logging):
 
     return local_llm
 
+def sql_mode(llm, pg_uri, schema, table_list, question) -> str:
+    db = SQLDatabase.from_uri(pg_uri, schema=schema, include_tables=table_list)
 
-def retrieval_qa_pipline(device_type, use_history, promptTemplate_type="llama"):
+    # Create db chain
+    QUERY = """
+    {question}
     """
-    Initializes and returns a retrieval-based Question Answering (QA) pipeline.
-
-    This function sets up a QA system that retrieves relevant information using embeddings
-    from the HuggingFace library. It then answers questions based on the retrieved information.
-
-    Parameters:
-    - device_type (str): Specifies the type of device where the model will run, e.g., 'cpu', 'cuda', etc.
-    - use_history (bool): Flag to determine whether to use chat history or not.
-
-    Returns:
-    - RetrievalQA: An initialized retrieval-based QA system.
-
-    Notes:
-    - The function uses embeddings from the HuggingFace library, either instruction-based or regular.
-    - The Chroma class is used to load a vector store containing pre-computed embeddings.
-    - The retriever fetches relevant documents or data based on a query.
-    - The prompt and memory, obtained from the `get_prompt_template` function, might be used in the QA system.
-    - The model is loaded onto the specified device using its ID and basename.
-    - The QA system retrieves relevant documents using the retriever and then answers questions based on those documents.
-    """
-
-    """
-    (1) Chooses an appropriate langchain library based on the enbedding model name.  Matching code is contained within ingest.py.
-    
-    (2) Provides additional arguments for instructor and BGE models to improve results, pursuant to the instructions contained on
-    their respective huggingface repository, project page or github repository.
-    """
-
-    embeddings = get_embeddings(device_type)
-
-    logging.info(f"Loaded embeddings from {EMBEDDING_MODEL_NAME}")
-
-    # load the vectorstore
-    db = Chroma(persist_directory=PERSIST_DIRECTORY, embedding_function=embeddings, client_settings=CHROMA_SETTINGS)
-    retriever = db.as_retriever()
-
-    # get the prompt template and memory if set by the user.
-    prompt, memory = get_prompt_template(promptTemplate_type=promptTemplate_type, history=use_history)
-
-    # load the llm pipeline
-    llm = load_model(device_type, model_id=MODEL_ID, model_basename=MODEL_BASENAME, LOGGING=logging)
-
-    if use_history:
-        qa = RetrievalQA.from_chain_type(
-            llm=llm,
-            chain_type="stuff",  # try other chains types as well. refine, map_reduce, map_rerank
-            retriever=retriever,
-            return_source_documents=True,  # verbose=True,
-            callbacks=callback_manager,
-            chain_type_kwargs={"prompt": prompt, "memory": memory},
-        )
-    else:
-        qa = RetrievalQA.from_chain_type(
-            llm=llm,
-            chain_type="stuff",  # try other chains types as well. refine, map_reduce, map_rerank
-            retriever=retriever,
-            return_source_documents=True,  # verbose=True,
-            callbacks=callback_manager,
-            chain_type_kwargs={
-                "prompt": prompt,
-            },
-        )
-
-    return qa
+    # Setup the database chain
+    # use return_direct=True to directly return the output of the SQL query without any additional formatting
+    db_chain = SQLDatabaseChain(llm=llm, database=db, verbose=True, return_intermediate_steps=True)
+
+    question = QUERY.format(question=question)
+    return db_chain(question)
+
+def url_mode(llm, query, url, embeddings, file_type=None) -> str:
+    loader = UnstructuredURLLoader(urls=[url])
+    loaded_docs = loader.load()
+    text_splitter = RecursiveCharacterTextSplitter(chunk_size=250, chunk_overlap=50)
+    chunked_docs = text_splitter.split_documents(loaded_docs)
+    PERSIST_DIRECTORY = f"{ROOT_DIRECTORY}/DB1"
+    CHROMA_SETTINGS = Settings(
+    chroma_db_impl="duckdb+parquet", persist_directory=PERSIST_DIRECTORY, anonymized_telemetry=False
+)
+    db1 = Chroma.from_documents(
+        chunked_docs,
+        embeddings,
+        persist_directory=PERSIST_DIRECTORY,
+        client_settings=CHROMA_SETTINGS,
+    )
+    # db1.persist()
+    # db1 = None
+    retriever = db1.as_retriever()
+    qa = RetrievalQA.from_chain_type(llm=llm, chain_type="stuff", retriever=retriever, return_source_documents=True)
+    res = qa(query)
+    return res
 
 
 # chose device typ to run on as well as to show source documents.
 @click.command()
 @click.option(
     "--device_type",
-    default="cuda" if torch.cuda.is_available() else "cpu",
+    default="cuda",
     type=click.Choice(
         [
             "cpu",
@@ -202,80 +215,185 @@ def retrieval_qa_pipline(device_type, use_history, promptTemplate_type="llama"):
     help="Show sources along with answers (Default is False)",
 )
 @click.option(
-    "--use_history",
-    "-h",
-    is_flag=True,
-    help="Use history (Default is False)",
-)
-@click.option(
-    "--model_type",
-    default="llama3",
+    "--mode",
+    default="doc",
     type=click.Choice(
-        ["llama3", "llama", "mistral", "non_llama"],
+        [
+            "doc",
+            "sql",
+            "web",
+        ],
     ),
-    help="model type, llama3, llama, mistral or non_llama",
+    help="Run mode can be doc, sql or web. (Default is doc)",
 )
-@click.option(
-    "--save_qa",
-    is_flag=True,
-    help="whether to save Q&A pairs to a CSV file (Default is False)",
-)
-def main(device_type, show_sources, use_history, model_type, save_qa):
+def main(device_type, show_sources, mode):
     """
-    Implements the main information retrieval task for a localGPT.
-
-    This function sets up the QA system by loading the necessary embeddings, vectorstore, and LLM model.
-    It then enters an interactive loop where the user can input queries and receive answers. Optionally,
-    the source documents used to derive the answers can also be displayed.
+    This function implements the information retrieval task.
 
-    Parameters:
-    - device_type (str): Specifies the type of device where the model will run, e.g., 'cpu', 'mps', 'cuda', etc.
-    - show_sources (bool): Flag to determine whether to display the source documents used for answering.
-    - use_history (bool): Flag to determine whether to use chat history or not.
-
-    Notes:
-    - Logging information includes the device type, whether source documents are displayed, and the use of history.
-    - If the models directory does not exist, it creates a new one to store models.
-    - The user can exit the interactive loop by entering "exit".
-    - The source documents are displayed if the show_sources flag is set to True.
 
+    1. Loads an embedding model, can be HuggingFaceInstructEmbeddings or HuggingFaceEmbeddings
+    2. Loads the existing vectorestore that was created by inget.py
+    3. Loads the local LLM using load_model function - You can now set different LLMs.
+    4. Setup the Question Answer retreival chain.
+    5. Question answers.
     """
 
     logging.info(f"Running on: {device_type}")
     logging.info(f"Display Source Documents set to: {show_sources}")
-    logging.info(f"Use history set to: {use_history}")
-
-    # check if models directory do not exist, create a new one and store models here.
-    if not os.path.exists(MODELS_PATH):
-        os.mkdir(MODELS_PATH)
-
-    qa = retrieval_qa_pipline(device_type, use_history, promptTemplate_type=model_type)
-    # Interactive questions and answers
-    while True:
-        query = input("\nEnter a query: ")
-        if query == "exit":
-            break
-        # Get the answer from the chain
-        res = qa(query)
-        answer, docs = res["result"], res["source_documents"]
-
-        # Print the result
-        print("\n\n> Question:")
-        print(query)
-        print("\n> Answer:")
-        print(answer)
-
-        if show_sources:  # this is a flag that you can set to disable showing answers.
-            # # Print the relevant sources used for the answer
-            print("----------------------------------SOURCE DOCUMENTS---------------------------")
-            for document in docs:
-                print("\n> " + document.metadata["source"] + ":")
-                print(document.page_content)
-            print("----------------------------------SOURCE DOCUMENTS---------------------------")
-
-        # Log the Q&A to CSV only if save_qa is True
-        if save_qa:
-            utils.log_to_csv(query, answer)
+
+    embeddings = HuggingFaceInstructEmbeddings(model_name=EMBEDDING_MODEL_NAME, model_kwargs={"device": device_type})
+
+    # uncomment the following line if you used HuggingFaceEmbeddings in the ingest.py
+    # embeddings = HuggingFaceEmbeddings(model_name=EMBEDDING_MODEL_NAME)
+
+    # load the LLM for generating Natural Language responses
+
+    # for HF models
+    model_id = "TheBloke/vicuna-7B-1.1-HF"
+    model_basename = None
+    # model_id = "TheBloke/Wizard-Vicuna-7B-Uncensored-HF"
+    # model_id = "TheBloke/guanaco-7B-HF"
+    # model_id = 'NousResearch/Nous-Hermes-13b' # Requires ~ 23GB VRAM. Using STransformers
+    # alongside will 100% create OOM on 24GB cards.
+    # llm = load_model(device_type, model_id=model_id)
+
+    if device_type == "cuda":
+        # for GPTQ (quantized) models
+        # model_id = "TheBloke/Nous-Hermes-13B-GPTQ"
+        # model_basename = "nous-hermes-13b-GPTQ-4bit-128g.no-act.order"
+        # model_id = "TheBloke/WizardLM-30B-Uncensored-GPTQ"
+        # model_basename = "WizardLM-30B-Uncensored-GPTQ-4bit.act-order.safetensors" # Requires
+        # ~21GB VRAM. Using STransformers alongside can potentially create OOM on 24GB cards.
+        # model_id = "TheBloke/wizardLM-7B-GPTQ"
+        # model_basename = "wizardLM-7B-GPTQ-4bit.compat.no-act-order.safetensors"
+        # model_id = "TheBloke/WizardLM-7B-uncensored-GPTQ"
+        # model_basename = "WizardLM-7B-uncensored-GPTQ-4bit-128g.compat.no-act-order.safetensors"
+        # model_id = "TheBloke/Nous-Hermes-Llama2-GPTQ"
+        model_id = "TheBloke/Nous-Hermes-13B-GPTQ"
+        model_basename = "model.safetensors"  
+    else:
+        # for GGML (quantized cpu+gpu+mps) models - check if they support llama.cpp
+        # model_id = "TheBloke/wizard-vicuna-13B-GGML"
+        # model_basename = "wizard-vicuna-13B.ggmlv3.q4_0.bin"
+        # model_basename = "wizard-vicuna-13B.ggmlv3.q6_K.bin"
+        # model_basename = "wizard-vicuna-13B.ggmlv3.q2_K.bin"
+        # model_id = "TheBloke/orca_mini_3B-GGML"
+        # model_basename = "orca-mini-3b.ggmlv3.q4_0.bin"
+        # model_id = "TheBloke/orca_mini_v2_7B-GGML"
+        # model_basename = "orca-mini-v2_7b.ggmlv3.q3_K_M.bin"
+        # model_id = "TheBloke/Wizard-Vicuna-7B-Uncensored-GGML"
+        # model_basename = "Wizard-Vicuna-7B-Uncensored.ggmlv3.q4_K_S.bin"
+        # model_basename = "Wizard-Vicuna-7B-Uncensored.ggmlv3.q3_K_S.bin"
+        # model_id = "TheBloke/Llama-2-7B-Chat-GGML"
+        # model_basename = "llama-2-7b-chat.ggmlv3.q3_K_S.bin"
+        model_id = "TheBloke/Nous-Hermes-Llama2-GGML"
+        model_basename = "nous-hermes-llama2-13b.ggmlv3.q3_K_S.bin"
+
+    if llm_type == "open_source":
+        llm = load_model(device_type, model_id=model_id, model_basename=model_basename)
+    else:
+        llm = OpenAI(temperature=0, openai_api_key=OPENAI_API_KEY)
+
+    if mode == "doc":
+        # load the vectorstore
+        db = Chroma(
+            persist_directory=PERSIST_DIRECTORY,
+            embedding_function=embeddings,
+            client_settings=CHROMA_SETTINGS,
+        )
+        retriever = db.as_retriever()
+        qa = RetrievalQA.from_chain_type(llm=llm, chain_type="stuff", retriever=retriever, return_source_documents=True)
+        # Interactive questions and answers
+        while True:
+            query = input("\nEnter a query: ")
+            if query == "exit":
+                break
+            # Get the answer from the chain
+            res = qa(query)
+            answer, docs = res["result"], res["source_documents"]
+
+            # Print the result
+            print("\n\n> Question:")
+            print(query)
+            print("\n> Answer:")
+            print(answer)
+
+            if show_sources:  # this is a flag that you can set to disable showing answers.
+                # # Print the relevant sources used for the answer
+                print("----------------------------------SOURCE DOCUMENTS---------------------------")
+                for document in docs:
+                    print("\n> " + document.metadata["source"] + ":")
+                    print(document.page_content)
+                print("----------------------------------SOURCE DOCUMENTS---------------------------")
+    elif mode == "sql":
+        while True:
+            query = input("\nEnter a query: ")
+            if query == "exit":
+                break        
+            schema = input("Enter dataset: ")
+            tables = input("Enter tables: ")
+            table_list = tables.replace(" ", "").split(",")
+            result = sql_mode(llm, pg_uri, schema, table_list, query)
+            print(result["intermediate_steps"][-1])
+    elif mode == "external_doc":
+        PERSIST_DIRECTORY = f"{ROOT_DIRECTORY}/DB1"
+        CHROMA_SETTINGS = Settings(
+        chroma_db_impl="duckdb+parquet", persist_directory=PERSIST_DIRECTORY, anonymized_telemetry=False
+    )    
+        # load the vectorstore
+        db1 = Chroma(
+            persist_directory=PERSIST_DIRECTORY,
+            embedding_function=embeddings,
+            client_settings=CHROMA_SETTINGS,
+        )
+        retriever = db1.as_retriever()
+        qa = RetrievalQA.from_chain_type(llm=llm, chain_type="stuff", retriever=retriever, return_source_documents=True)
+        # Interactive questions and answers
+        while True:
+            query = input("\nEnter a query: ")
+            if query == "exit":
+                break
+            # Get the answer from the chain
+            res = qa(query)
+            answer, docs = res["result"], res["source_documents"]
+
+            # Print the result
+            print("\n\n> Question:")
+            print(query)
+            print("\n> Answer:")
+            print(answer)
+
+            if show_sources:  # this is a flag that you can set to disable showing answers.
+                # # Print the relevant sources used for the answer
+                print("----------------------------------SOURCE DOCUMENTS---------------------------")
+                for document in docs:
+                    print("\n> " + document.metadata["source"] + ":")
+                    print(document.page_content)
+                print("----------------------------------SOURCE DOCUMENTS---------------------------")
+    
+    else:
+        while True:
+            query = input("\nEnter a query: ")
+            if query == "exit":
+                break   
+            url = input("\nEnter URL: ")
+            res = url_mode(llm, query, url, embeddings)
+            answer, docs = res["result"], res["source_documents"]
+
+            # Print the result
+            print("\n\n> Question:")
+            print(query)
+            print("\n> Answer:")
+            print(answer)
+
+            if show_sources:  # this is a flag that you can set to disable showing answers.
+                # # Print the relevant sources used for the answer
+                print("----------------------------------SOURCE DOCUMENTS---------------------------")
+                for document in docs:
+                    print("\n> " + document.metadata["source"] + ":")
+                    print(document.page_content)
+                print("----------------------------------SOURCE DOCUMENTS---------------------------")
+
 
 
 if __name__ == "__main__":
diff --git a/run_localGPT_API.py b/run_localGPT_API.py
index b345612..76e9521 100644
--- a/run_localGPT_API.py
+++ b/run_localGPT_API.py
@@ -2,41 +2,64 @@ import logging
 import os
 import shutil
 import subprocess
-import argparse
 
 import torch
+from auto_gptq import AutoGPTQForCausalLM
 from flask import Flask, jsonify, request
+from huggingface_hub import hf_hub_download
 from langchain.chains import RetrievalQA
 from langchain.embeddings import HuggingFaceInstructEmbeddings
 
 # from langchain.embeddings import HuggingFaceEmbeddings
-from run_localGPT import load_model
-from prompt_template_utils import get_prompt_template
+from langchain.llms import HuggingFacePipeline, LlamaCpp
 
 # from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler
 from langchain.vectorstores import Chroma
+from transformers import (
+    AutoModelForCausalLM,
+    AutoTokenizer,
+    GenerationConfig,
+    LlamaForCausalLM,
+    LlamaTokenizer,
+    pipeline,
+)
 from werkzeug.utils import secure_filename
 
-from constants import CHROMA_SETTINGS, EMBEDDING_MODEL_NAME, PERSIST_DIRECTORY, MODEL_ID, MODEL_BASENAME
-
-# API queue addition
-from threading import Lock
-
-request_lock = Lock()
-
-
-if torch.backends.mps.is_available():
-    DEVICE_TYPE = "mps"
-elif torch.cuda.is_available():
-    DEVICE_TYPE = "cuda"
-else:
-    DEVICE_TYPE = "cpu"
+from constants import CHROMA_SETTINGS, EMBEDDING_MODEL_NAME, PERSIST_DIRECTORY, ROOT_DIRECTORY
+from chromadb.config import Settings
 
+from langchain import OpenAI, SQLDatabase, SQLDatabaseChain
+from langchain.document_loaders import UnstructuredURLLoader
+from langchain.text_splitter import Language, RecursiveCharacterTextSplitter
+# from youtube_transcript_api import YouTubeTranscriptApi
+import os
+from dotenv import load_dotenv
+load_dotenv()
+user_name = os.getenv('user_name')
+user_pwd = os.getenv('user_pwd')
+llm_type = os.getenv('llm_type')
+OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')
+gcp_project = os.getenv('gcp_project')
+host_name = os.getenv('host_name')
+port = 5432
+database = 'rain'
+pronto_uri = f"postgresql+psycopg2://{user_name}:{user_pwd}@{host_name}:{port}/{database}"
+jdbcHostname = os.getenv('jdbcHostname')
+jdbcUsername = os.getenv('jdbcUsername')
+jdbcPassword = os.getenv('jdbcPassword')
+dbms = os.getenv('dbms')
+omnifin_uri = f'mysql+pymysql://{jdbcUsername}:{jdbcPassword}@{jdbcHostname}/{dbms}'
+
+device_type = os.getenv('device_type')
+# temporary, just to check questions entered
+logging.basicConfig(filename="logs.txt", filemode='a', 
+    format="%(asctime)s - %(levelname)s - %(filename)s:%(lineno)s - %(message)s", level=logging.INFO
+)
 SHOW_SOURCES = True
-logging.info(f"Running on: {DEVICE_TYPE}")
+logging.info(f"Running on: {device_type}")
 logging.info(f"Display Source Documents set to: {SHOW_SOURCES}")
 
-EMBEDDINGS = HuggingFaceInstructEmbeddings(model_name=EMBEDDING_MODEL_NAME, model_kwargs={"device": DEVICE_TYPE})
+EMBEDDINGS = HuggingFaceInstructEmbeddings(model_name=EMBEDDING_MODEL_NAME, model_kwargs={"device": device_type})
 
 # uncomment the following line if you used HuggingFaceEmbeddings in the ingest.py
 # EMBEDDINGS = HuggingFaceEmbeddings(model_name=EMBEDDING_MODEL_NAME)
@@ -68,17 +91,217 @@ DB = Chroma(
 
 RETRIEVER = DB.as_retriever()
 
-LLM = load_model(device_type=DEVICE_TYPE, model_id=MODEL_ID, model_basename=MODEL_BASENAME)
-prompt, memory = get_prompt_template(promptTemplate_type="llama", history=False)
+
+# load the LLM for generating Natural Language responses
+def load_model(device_type, model_id, model_basename=None):
+    """
+    Select a model for text generation using the HuggingFace library.
+    If you are running this for the first time, it will download a model for you.
+    subsequent runs will use the model from the disk.
+
+    Args:
+        device_type (str): Type of device to use, e.g., "cuda" for GPU or "cpu" for CPU.
+        model_id (str): Identifier of the model to load from HuggingFace's model hub.
+        model_basename (str, optional): Basename of the model if using quantized models.
+            Defaults to None.
+
+    Returns:
+        HuggingFacePipeline: A pipeline object for text generation using the loaded model.
+
+    Raises:
+        ValueError: If an unsupported model or device type is provided.
+    """
+    logging.info(f"Loading Model: {model_id}, on: {device_type}")
+    # logging.info("This action can take a few minutes!")
+
+    if model_basename is not None:
+        if device_type.lower() in ["cpu", "mps"]:
+            logging.info("Using Llamacpp for quantized models")
+            model_path = hf_hub_download(repo_id=model_id, filename=model_basename)
+            if device_type.lower() == "mps":
+                return LlamaCpp(
+                    model_path=model_path,
+                    n_ctx=2048,
+                    max_tokens=2048,
+                    temperature=0,
+                    repeat_penalty=1.15,
+                    n_gpu_layers=1000,
+                )
+            return LlamaCpp(model_path=model_path, n_ctx=2048, max_tokens=2048, temperature=0, repeat_penalty=1.15)
+
+        else:
+            # The code supports all huggingface models that ends with GPTQ and have some variation
+            # of .no-act.order or .safetensors in their HF repo.
+            logging.info("Using AutoGPTQForCausalLM for quantized models")
+
+            if ".safetensors" in model_basename:
+                # Remove the ".safetensors" ending if present
+                model_basename = model_basename.replace(".safetensors", "")
+
+            tokenizer = AutoTokenizer.from_pretrained(model_id, use_fast=True)
+            logging.info("Tokenizer loaded")
+
+            model = AutoGPTQForCausalLM.from_quantized(
+                model_id,
+                model_basename=model_basename,
+                use_safetensors=True,
+                trust_remote_code=True,
+                device="cuda:0",
+                use_triton=False,
+                quantize_config=None,
+            )
+    elif (
+        device_type.lower() == "cuda"
+    ):  # The code supports all huggingface models that ends with -HF or which have a .bin
+        # file in their HF repo.
+        logging.info("Using AutoModelForCausalLM for full models")
+        tokenizer = AutoTokenizer.from_pretrained(model_id)
+        logging.info("Tokenizer loaded")
+
+        model = AutoModelForCausalLM.from_pretrained(
+            model_id,
+            device_map="auto",
+            torch_dtype=torch.float16,
+            low_cpu_mem_usage=True,
+            trust_remote_code=True,
+            # max_memory={0: "15GB"} # Uncomment this line with you encounter CUDA out of memory errors
+        )
+        model.tie_weights()
+    else:
+        logging.info("Using LlamaTokenizer")
+        tokenizer = LlamaTokenizer.from_pretrained(model_id)
+        model = LlamaForCausalLM.from_pretrained(model_id)
+
+    # Load configuration from the model to avoid warnings
+    generation_config = GenerationConfig.from_pretrained(model_id)
+    # see here for details:
+    # https://huggingface.co/docs/transformers/
+    # main_classes/text_generation#transformers.GenerationConfig.from_pretrained.returns
+
+    # Create a pipeline for text generation
+    pipe = pipeline(
+        "text-generation",
+        model=model,
+        tokenizer=tokenizer,
+        max_length=2048,
+        temperature=0,
+        top_p=0.95,
+        repetition_penalty=1.15,
+        generation_config=generation_config,
+    )
+
+    local_llm = HuggingFacePipeline(pipeline=pipe)
+    logging.info("Local LLM Loaded")
+
+    return local_llm
+
+def sql_mode(llm, pg_uri, schema, table_list, question) -> str:
+        
+    db = SQLDatabase.from_uri(pg_uri, sample_rows_in_table_info=2, schema=schema, include_tables=table_list)
+    db._schema=None
+    # Create db chain
+    if pg_uri == pronto_uri:
+        QUERY = """
+        Given an input question, first create a syntactically correct postgresql query to run, then look at the results of the query and return the answer.
+        Use the following format:
+
+        Question: Question here
+        SQLQuery: SQL Query to run
+        SQLResult: Result of the SQLQuery
+        Answer: Final answer here
+
+        {question}
+        """
+    else:
+        QUERY = """
+        Given an input question, first create a syntactically correct mysql query to run, then look at the results of the query and return the answer.
+        Use the following format:
+
+        Question: Question here
+        SQLQuery: SQL Query to run
+        SQLResult: Result of the SQLQuery
+        Answer: Final answer here
+
+        {question}
+        """
+    # Setup the database chain
+    # use return_direct=True to directly return the output of the SQL query without any additional formatting
+    db_chain = SQLDatabaseChain(llm=llm, database=db, verbose=True, return_intermediate_steps=True)
+    # question = QUERY.format(question=question)
+    return db_chain(question)
+
+def url_mode(llm, query, url, embeddings, file_type=None) -> str:
+    loader = UnstructuredURLLoader(urls=[url])
+    loaded_docs = loader.load()
+    text_splitter = RecursiveCharacterTextSplitter(chunk_size=250, chunk_overlap=50)
+    chunked_docs = text_splitter.split_documents(loaded_docs)
+    PERSIST_DIRECTORY = f"{ROOT_DIRECTORY}/DB1"
+    CHROMA_SETTINGS = Settings(
+        chroma_db_impl="duckdb+parquet", persist_directory=PERSIST_DIRECTORY, anonymized_telemetry=False
+    )
+    db1 = Chroma.from_documents(
+        chunked_docs,
+        embeddings,
+        persist_directory=PERSIST_DIRECTORY,
+        client_settings=CHROMA_SETTINGS,
+    )
+    # db1.persist()
+    # db1 = None
+    retriever = db1.as_retriever()
+    qa = RetrievalQA.from_chain_type(llm=llm, chain_type="stuff", retriever=retriever, return_source_documents=True)
+    res = qa(query)
+    return res
+
+
+# for HF models
+model_id = "TheBloke/vicuna-7B-1.1-HF"
+model_basename = None
+# model_id = "TheBloke/Wizard-Vicuna-7B-Uncensored-HF"
+# model_id = "TheBloke/guanaco-7B-HF"
+# model_id = 'NousResearch/Nous-Hermes-13b' # Requires ~ 23GB VRAM. Using STransformers
+# alongside will 100% create OOM on 24GB cards.
+# llm = load_model(device_type, model_id=model_id)
+
+if device_type == "cuda":
+    # for GPTQ (quantized) models
+    # model_id = "TheBloke/Nous-Hermes-13B-GPTQ"
+    # model_basename = "nous-hermes-13b-GPTQ-4bit-128g.no-act.order"
+    # model_id = "TheBloke/WizardLM-30B-Uncensored-GPTQ"
+    # model_basename = "WizardLM-30B-Uncensored-GPTQ-4bit.act-order.safetensors" # Requires
+    # ~21GB VRAM. Using STransformers alongside can potentially create OOM on 24GB cards.
+    # model_id = "TheBloke/wizardLM-7B-GPTQ"
+    # model_basename = "wizardLM-7B-GPTQ-4bit.compat.no-act-order.safetensors"
+    # model_id = "TheBloke/WizardLM-7B-uncensored-GPTQ"
+    # model_basename = "WizardLM-7B-uncensored-GPTQ-4bit-128g.compat.no-act-order.safetensors"
+    # model_id = "TheBloke/Nous-Hermes-Llama2-GPTQ"
+    model_id = "TheBloke/Nous-Hermes-13B-GPTQ"
+    model_basename = "model.safetensors"   
+
+if device_type != "cuda":
+    # for GGML (quantized cpu+gpu+mps) models - check if they support llama.cpp
+    # model_id = "TheBloke/wizard-vicuna-13B-GGML"
+    # model_basename = "wizard-vicuna-13B.ggmlv3.q4_0.bin"
+    # model_basename = "wizard-vicuna-13B.ggmlv3.q6_K.bin"
+    # model_basename = "wizard-vicuna-13B.ggmlv3.q2_K.bin"
+    # model_id = "TheBloke/orca_mini_3B-GGML"
+    # model_basename = "orca-mini-3b.ggmlv3.q4_0.bin"
+    # model_id = "TheBloke/orca_mini_v2_7B-GGML"
+    # model_basename = "orca-mini-v2_7b.ggmlv3.q3_K_M.bin"
+    # model_id = "TheBloke/Wizard-Vicuna-7B-Uncensored-GGML"
+    # model_basename = "Wizard-Vicuna-7B-Uncensored.ggmlv3.q4_K_S.bin"
+    # model_basename = "Wizard-Vicuna-7B-Uncensored.ggmlv3.q3_K_S.bin"
+    # model_id = "TheBloke/Llama-2-7B-Chat-GGML"
+    # model_basename = "llama-2-7b-chat.ggmlv3.q3_K_S.bin"
+    model_id = "TheBloke/Nous-Hermes-Llama2-GGML"
+    model_basename = "nous-hermes-llama2-13b.ggmlv3.q3_K_S.bin"
+
+if llm_type == "open_source":
+    LLM = load_model(device_type, model_id=model_id, model_basename=model_basename)
+else:
+    LLM = OpenAI(temperature=0, openai_api_key=OPENAI_API_KEY)
 
 QA = RetrievalQA.from_chain_type(
-    llm=LLM,
-    chain_type="stuff",
-    retriever=RETRIEVER,
-    return_source_documents=SHOW_SOURCES,
-    chain_type_kwargs={
-        "prompt": prompt,
-    },
+    llm=LLM, chain_type="stuff", retriever=RETRIEVER, return_source_documents=SHOW_SOURCES
 )
 
 app = Flask(__name__)
@@ -86,7 +309,7 @@ app = Flask(__name__)
 
 @app.route("/api/delete_source", methods=["GET"])
 def delete_source_route():
-    folder_name = "SOURCE_DOCUMENTS"
+    folder_name = "SOURCE_DOCUMENTS_EXTERNAL"
 
     if os.path.exists(folder_name):
         shutil.rmtree(folder_name)
@@ -105,7 +328,7 @@ def save_document_route():
         return "No selected file", 400
     if file:
         filename = secure_filename(file.filename)
-        folder_path = "SOURCE_DOCUMENTS"
+        folder_path = "SOURCE_DOCUMENTS_EXTERNAL"
         if not os.path.exists(folder_path):
             os.makedirs(folder_path)
         file_path = os.path.join(folder_path, filename)
@@ -118,6 +341,7 @@ def run_ingest_route():
     global DB
     global RETRIEVER
     global QA
+    PERSIST_DIRECTORY = f"{ROOT_DIRECTORY}/DB1"
     try:
         if os.path.exists(PERSIST_DIRECTORY):
             try:
@@ -127,31 +351,27 @@ def run_ingest_route():
         else:
             print("The directory does not exist")
 
-        run_langest_commands = ["python", "ingest.py"]
-        if DEVICE_TYPE == "cpu":
+        run_langest_commands = ["python", "ingest_external.py"]
+        if device_type != "cuda":
             run_langest_commands.append("--device_type")
-            run_langest_commands.append(DEVICE_TYPE)
-
+            run_langest_commands.append(device_type)
+            
         result = subprocess.run(run_langest_commands, capture_output=True)
         if result.returncode != 0:
             return "Script execution failed: {}".format(result.stderr.decode("utf-8")), 500
         # load the vectorstore
+        CHROMA_SETTINGS = Settings(
+            chroma_db_impl="duckdb+parquet", persist_directory=PERSIST_DIRECTORY, anonymized_telemetry=False
+        )
         DB = Chroma(
             persist_directory=PERSIST_DIRECTORY,
             embedding_function=EMBEDDINGS,
             client_settings=CHROMA_SETTINGS,
         )
         RETRIEVER = DB.as_retriever()
-        prompt, memory = get_prompt_template(promptTemplate_type="llama", history=False)
 
         QA = RetrievalQA.from_chain_type(
-            llm=LLM,
-            chain_type="stuff",
-            retriever=RETRIEVER,
-            return_source_documents=SHOW_SOURCES,
-            chain_type_kwargs={
-                "prompt": prompt,
-            },
+            llm=LLM, chain_type="stuff", retriever=RETRIEVER, return_source_documents=SHOW_SOURCES
         )
         return "Script executed successfully: {}".format(result.stdout.decode("utf-8")), 200
     except Exception as e:
@@ -160,14 +380,57 @@ def run_ingest_route():
 
 @app.route("/api/prompt_route", methods=["GET", "POST"])
 def prompt_route():
-    global QA
-    global request_lock  # Make sure to use the global lock instance
     user_prompt = request.form.get("user_prompt")
+    logging.info(f'User Prompt: {user_prompt}')
     if user_prompt:
-        # Acquire the lock before processing the prompt
-        with request_lock:
-            # print(f'User Prompt: {user_prompt}')              
-            # Get the answer from the chain
+        # print(f'User Prompt: {user_prompt}')
+        # Get the answer from the chain
+        if  user_prompt.startswith('E:'):
+            # load the vectorstore            
+            PERSIST_DIRECTORY = f"{ROOT_DIRECTORY}/DB1"
+            CHROMA_SETTINGS = Settings(
+                chroma_db_impl="duckdb+parquet", persist_directory=PERSIST_DIRECTORY, anonymized_telemetry=False
+            )
+            DB1 = Chroma(
+                persist_directory=PERSIST_DIRECTORY,
+                embedding_function=EMBEDDINGS,
+                client_settings=CHROMA_SETTINGS,
+            )
+            RETRIEVER = DB1.as_retriever()
+
+            QA = RetrievalQA.from_chain_type(
+                llm=LLM, chain_type="stuff", retriever=RETRIEVER, return_source_documents=SHOW_SOURCES
+            )
+            res = QA(user_prompt)
+            answer, docs = res["result"], res["source_documents"]
+
+            prompt_response_dict = {
+                "Prompt": user_prompt,
+                "Answer": answer,
+            }
+
+            prompt_response_dict["Sources"] = []
+            for document in docs:
+                prompt_response_dict["Sources"].append(
+                    (os.path.basename(str(document.metadata["source"])), str(document.page_content))
+                )
+
+            return jsonify(prompt_response_dict), 200
+        else:
+            # load the vectorstore            
+            PERSIST_DIRECTORY = f"{ROOT_DIRECTORY}/DB"
+            CHROMA_SETTINGS = Settings(
+                chroma_db_impl="duckdb+parquet", persist_directory=PERSIST_DIRECTORY, anonymized_telemetry=False
+            )
+            DB = Chroma(
+                persist_directory=PERSIST_DIRECTORY,
+                embedding_function=EMBEDDINGS,
+                client_settings=CHROMA_SETTINGS,
+            )
+            RETRIEVER = DB.as_retriever()
+            QA = RetrievalQA.from_chain_type(
+                llm=LLM, chain_type="stuff", retriever=RETRIEVER, return_source_documents=SHOW_SOURCES
+            )
             res = QA(user_prompt)
             answer, docs = res["result"], res["source_documents"]
 
@@ -182,25 +445,80 @@ def prompt_route():
                     (os.path.basename(str(document.metadata["source"])), str(document.page_content))
                 )
 
+            return jsonify(prompt_response_dict), 200           
+    else:
+        return "No user prompt received", 400
+    
+@app.route("/api/sql_route", methods=["GET", "POST"])
+def sql_route():
+    global LLM
+    user_prompt = request.form.get("user_prompt")
+    logging.info(f'User Prompt: {user_prompt}')
+    data_list = user_prompt.split(";")
+    user_prompt = data_list[0]
+    try:
+        db_name = data_list[-3].replace(" ", "")
+    except:
+        db_name = "omnifin"
+    try:
+        schema = data_list[-2].replace(" ", "")
+    except:
+        schema = "beefin_uat"
+    tables = data_list[-1].replace(" ", "")
+    if db_name.lower() == 'pronto':
+        pg_uri = pronto_uri
+    else:
+        pg_uri = omnifin_uri
+    if user_prompt:
+        # print(f'User Prompt: {user_prompt}')
+        # Get the answer from the chain
+        result = sql_mode(LLM, pg_uri, schema, [tables], user_prompt)
+        answer, docs = result["intermediate_steps"][-1], result["intermediate_steps"][1]
+
+        prompt_response_dict = {
+            "Prompt": user_prompt,
+            "Answer": answer,
+        }
+
+        prompt_response_dict["Sources"] = [("SQL Query", str(docs))]
+
         return jsonify(prompt_response_dict), 200
     else:
         return "No user prompt received", 400
+    
+@app.route("/api/web_route", methods=["GET", "POST"])
+def web_route():
+    global LLM
+    global EMBEDDINGS
+    user_prompt = request.form.get("user_prompt")
+    logging.info(f'User Prompt: {user_prompt}')
+    data_list = user_prompt.split(";")
+    user_prompt = data_list[0]
+    url = data_list[1]
+    if user_prompt:
+        # print(f'User Prompt: {user_prompt}')
+        # Get the answer from the chain
+        res = url_mode(LLM, user_prompt, url, EMBEDDINGS)
+        answer, docs = res["result"], res["source_documents"]
+
+        prompt_response_dict = {
+            "Prompt": user_prompt,
+            "Answer": answer,
+        }
+
+        prompt_response_dict["Sources"] = []
+        for document in docs:
+            prompt_response_dict["Sources"].append(
+                (os.path.basename(str(document.metadata["source"])), str(document.page_content))
+            )
 
+        return jsonify(prompt_response_dict), 200
+    else:
+        return "No user prompt received", 400
 
-if __name__ == "__main__":
-    parser = argparse.ArgumentParser()
-    parser.add_argument("--port", type=int, default=5110, help="Port to run the API on. Defaults to 5110.")
-    parser.add_argument(
-        "--host",
-        type=str,
-        default="127.0.0.1",
-        help="Host to run the UI on. Defaults to 127.0.0.1. "
-        "Set to 0.0.0.0 to make the UI externally "
-        "accessible from other devices.",
-    )
-    args = parser.parse_args()
 
-    logging.basicConfig(
+if __name__ == "__main__":
+    logging.basicConfig(filename="logs.txt", filemode='a',
         format="%(asctime)s - %(levelname)s - %(filename)s:%(lineno)s - %(message)s", level=logging.INFO
     )
-    app.run(debug=False, host=args.host, port=args.port)
+    app.run(debug=True, port=5110)
